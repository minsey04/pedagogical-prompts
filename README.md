# pedagogical-prompts

## AI vs Human-Crafted Questions: A Comparative Analysis of Prompt Design

This project is a personal investigation into a question I’ve been asking myself for a while:

> Can AI-generated questions really match the nuance and intention of those crafted by humans?

For over a year, I’ve worked as a part-time curriculum assistant, writing discussion questions for literature and media-based classes across a range of grade levels. I wasn’t just summarizing books — I was designing prompts meant to initiate conceptual thinking, invite debate, and explore meaning.

As generative AI became increasingly integrated into educational workflows, I began to notice how some teachers were relying on it to generate reading comprehension and discussion questions. This raised several questions for me, not just about performance but about the deeper structures of thinking and design:

- Are there consistent structural or semantic patterns that differentiate AI questions from human ones?
- How do human-designed questions vary in tone, structure, or intent?
- Can these differences be measured, visualized, and perhaps even modeled?

This is a small experiment built around the data I’ve quietly collected over time — not scraped, not synthesized, but written, curated, and labeled one question at a time.

## Dataset

The dataset consists of two distinct sources:

| Source          | Description |
|-----------------|-------------|
| Human-written   | 1500+ factual, conceptual, and debatable questions I wrote for over 30 books and essays |
| AI-generated    | Questions generated using consistent prompts via GPT-3.5 or GPT-4 |
| Metadata        | Book title, author, question type, length, source, date created |

The human dataset represents real-world usage: questions that were actually reviewed or used by teachers, written with context, constraints, and pedagogical goals in mind.

## Methods

### Linguistic Analysis

- Sentence length and word count
- Type-Token Ratio (TTR)
- Hapax Legomena ratio
- Readability scores (Flesch, Gunning Fog)

### Semantic Similarity and Distribution

- SBERT sentence embeddings
- UMAP for dimensionality reduction
- Clustering patterns and visual interpretation

### Statistical Testing

- Independent t-tests for average differences
- Mann-Whitney U tests for distribution shifts

The goal here is not just to evaluate quality, but to uncover the **implicit fingerprints** that may exist in the way questions are designed by humans versus generated by machines.

## Early Observations

Some preliminary patterns:

- AI questions tend to cluster tightly in semantic space, suggesting formulaic structures
- Human-written questions show wider lexical and semantic variation
- In readability and structure, human questions tend to be more adaptive to audience

These findings are exploratory — not absolute. But they begin to suggest where AI excels (efficiency, consistency) and where human input remains distinct (variation, contextual framing).

## Why This Matters

This is not a commercial or institutional project. It’s a personal inquiry into the intersection of language, education, and artificial intelligence.

By studying the subtle patterns in how questions are written — and what they reveal about design and reasoning — I hope to contribute in a small way to the broader conversation on the role of generative tools in learning.

## Tools Used

- Python (pandas, nltk, scipy, seaborn, matplotlib)
- sentence-transformers (`all-MiniLM-L6-v2`)
- UMAP
- OpenAI API (gpt-3.5-turbo)
- Google Sheets for data management

## Future Directions

- Build a classifier that distinguishes AI vs human prompts
- Use teacher surveys to triangulate perceptual feedback
- Expand the dataset with question context (passage excerpt, learning objective)

## Contact

If you’re interested in this kind of work — in question design, media literacy, or human-centered AI — I’d love to hear from you.

